{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5ac9a63",
   "metadata": {},
   "source": [
    "# 01 - Data Ingestion: MSFT Stock Data\n",
    "\n",
    "This notebook fetches historical stock data for Microsoft (MSFT) from Yahoo Finance and stores it in the Bronze layer of our Fabric Lakehouse using the medallion architecture.\n",
    "\n",
    "## Objectives:\n",
    "- Fetch MSFT historical stock data from yfinance API\n",
    "- Validate data quality and integrity\n",
    "- Store raw data in Bronze layer as Delta/Parquet format\n",
    "- Generate data summary statistics\n",
    "\n",
    "## Execution Schedule:\n",
    "Daily at 4:30 PM ET (after market close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40b107bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported\n",
      "Execution: 2025-12-04 13:44:24\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent / 'utils'))\n",
    "\n",
    "from data_loader import StockDataLoader, print_data_summary # type: ignore\n",
    "\n",
    "print(\"✓ Libraries imported\")\n",
    "print(f\"Execution: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eaebb3",
   "metadata": {},
   "source": [
    "## Step 1: Initialize Data Loader\n",
    "\n",
    "Configure the data loader with project settings from config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1b7b34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "  Ticker: MSFT\n",
      "  Date Range: 2020-01-01 to 2025-12-04\n",
      "  Primary API: yfinance\n"
     ]
    }
   ],
   "source": [
    "# Initialize data loader\n",
    "config_path = Path.cwd().parent / 'config' / 'config.json'\n",
    "loader = StockDataLoader(config_path=str(config_path))\n",
    "\n",
    "# Configuration\n",
    "TICKER = 'MSFT'\n",
    "START_DATE = '2020-01-01'  # Fetch from 2020 for sufficient history\n",
    "END_DATE = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  Ticker: {TICKER}\")\n",
    "print(f\"  Date Range: {START_DATE} to {END_DATE}\")\n",
    "print(f\"  Primary API: {loader.config['data_source']['primary_api']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1829bcdf",
   "metadata": {},
   "source": [
    "## Step 2: Fetch Historical Stock Data\n",
    "\n",
    "Download MSFT historical data from Yahoo Finance API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "682d2824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching MSFT data from 2020-01-01 to 2025-12-04...\n",
      "✓ Fetched 1489 records for MSFT\n",
      "\n",
      "First 5 rows:\n",
      "                       date        open        high         low       close  \\\n",
      "0 2020-01-02 00:00:00-05:00  150.758664  152.610150  150.331401  152.505707   \n",
      "1 2020-01-03 00:00:00-05:00  150.321872  151.869516  150.074998  150.606705   \n",
      "2 2020-01-06 00:00:00-05:00  149.144562  151.062519  148.603350  150.996048   \n",
      "3 2020-01-07 00:00:00-05:00  151.271365  151.603675  149.372403  149.619263   \n",
      "4 2020-01-08 00:00:00-05:00  150.901040  152.676579  149.970552  152.002441   \n",
      "\n",
      "     volume  dividends  stock_splits Ticker             FetchTimestamp  year  \\\n",
      "0  22622100        0.0           0.0   MSFT 2025-12-04 13:44:25.424026  2020   \n",
      "1  21116200        0.0           0.0   MSFT 2025-12-04 13:44:25.424026  2020   \n",
      "2  20813700        0.0           0.0   MSFT 2025-12-04 13:44:25.424026  2020   \n",
      "3  21634100        0.0           0.0   MSFT 2025-12-04 13:44:25.424026  2020   \n",
      "4  27746500        0.0           0.0   MSFT 2025-12-04 13:44:25.424026  2020   \n",
      "\n",
      "   month  \n",
      "0      1  \n",
      "1      1  \n",
      "2      1  \n",
      "3      1  \n",
      "4      1  \n",
      "\n",
      "Last 5 rows:\n",
      "                          date        open        high         low  \\\n",
      "1484 2025-11-26 00:00:00-05:00  486.309998  488.309998  481.200012   \n",
      "1485 2025-11-28 00:00:00-05:00  487.600006  492.630005  486.649994   \n",
      "1486 2025-12-01 00:00:00-05:00  488.440002  489.859985  484.649994   \n",
      "1487 2025-12-02 00:00:00-05:00  486.720001  493.500000  486.320007   \n",
      "1488 2025-12-03 00:00:00-05:00  476.320007  484.239990  475.200012   \n",
      "\n",
      "           close    volume  dividends  stock_splits Ticker  \\\n",
      "1484  485.500000  25709100        0.0           0.0   MSFT   \n",
      "1485  492.010010  14386700        0.0           0.0   MSFT   \n",
      "1486  486.739990  23964000        0.0           0.0   MSFT   \n",
      "1487  490.000000  19562700        0.0           0.0   MSFT   \n",
      "1488  477.730011  34562900        0.0           0.0   MSFT   \n",
      "\n",
      "                 FetchTimestamp  year  month  \n",
      "1484 2025-12-04 13:44:25.424026  2025     11  \n",
      "1485 2025-12-04 13:44:25.424026  2025     11  \n",
      "1486 2025-12-04 13:44:25.424026  2025     12  \n",
      "1487 2025-12-04 13:44:25.424026  2025     12  \n",
      "1488 2025-12-04 13:44:25.424026  2025     12  \n",
      "\n",
      "DataFrame shape: (1489, 12)\n",
      "Columns: ['date', 'open', 'high', 'low', 'close', 'volume', 'dividends', 'stock_splits', 'Ticker', 'FetchTimestamp', 'year', 'month']\n"
     ]
    }
   ],
   "source": [
    "# Fetch historical data\n",
    "df = loader.fetch_stock_data(\n",
    "    ticker=TICKER,\n",
    "    start_date=START_DATE,\n",
    "    end_date=END_DATE,\n",
    "    interval='1d'\n",
    ")\n",
    "\n",
    "# Display first and last few rows\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nLast 5 rows:\")\n",
    "print(df.tail())\n",
    "print(f\"\\nDataFrame shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4d25a4",
   "metadata": {},
   "source": [
    "## Step 3: Data Quality Validation\n",
    "\n",
    "Validate data integrity and quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "676118c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data validation passed\n",
      "✓ Data validation PASSED - Data is ready for storage\n"
     ]
    }
   ],
   "source": [
    "# Validate data\n",
    "is_valid, issues = loader.validate_data(df)\n",
    "\n",
    "if is_valid:\n",
    "    print(\"✓ Data validation PASSED - Data is ready for storage\")\n",
    "else:\n",
    "    print(\"✗ Data validation FAILED - Review issues before proceeding:\")\n",
    "    for issue in issues:\n",
    "        print(f\"  ! {issue}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdce4fcc",
   "metadata": {},
   "source": [
    "## Step 4: Generate Data Summary\n",
    "\n",
    "Calculate key statistics and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "730989b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATA SUMMARY\n",
      "============================================================\n",
      "Record Count:     1,489\n",
      "Date Range:       2020-01-02 to 2025-12-03\n",
      "Price Range:      $126.17 - $553.50\n",
      "Latest Close:     $477.73\n",
      "Avg Daily Volume: 27,640,162\n",
      "Total Return:     213.25%\n",
      "Avg Daily Return: 0.0942%\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate and display summary\n",
    "summary = loader.get_data_summary(df)\n",
    "print_data_summary(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c76b4c0",
   "metadata": {},
   "source": [
    "## Step 5: Store Data in Bronze Layer\n",
    "\n",
    "Save raw data to Lakehouse Bronze layer with partitioning\n",
    "\n",
    "**In Fabric**: Data will be stored as Delta table partitioned by year/month  \n",
    "**Local Development**: Data will be stored as Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99e4a3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1489 records to Bronze layer: bronze/stocks/stock_data\n",
      "✓ Data written successfully to bronze/stocks/stock_data\n",
      "\n",
      "============================================================\n",
      "DATA INGESTION COMPLETE\n",
      "============================================================\n",
      "Records stored: 1,489\n",
      "Storage location: stock_data\n",
      "Ready for transformation pipeline\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Define lakehouse path\n",
    "# In Fabric, this would be: /lakehouse/default/Files/\n",
    "LAKEHOUSE_PATH = str(Path.cwd().parent / 'data')  # Local path for development\n",
    "\n",
    "# Write to Bronze layer\n",
    "bronze_path = loader.write_to_bronze(df, LAKEHOUSE_PATH)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DATA INGESTION COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Records stored: {len(df):,}\")\n",
    "print(f\"Storage location: {Path(bronze_path).name}\")\n",
    "print(\"Ready for transformation pipeline\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401dc727",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Run Notebook 02**: Data transformation and cleaning\n",
    "2. **Schedule**: Configure daily execution at 4:30 PM ET in Fabric pipeline\n",
    "3. **Monitor**: Check data quality metrics regularly\n",
    "\n",
    "---\n",
    "**Note**: For incremental loads (daily updates), use `fetch_latest_data()` method instead of full historical fetch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
